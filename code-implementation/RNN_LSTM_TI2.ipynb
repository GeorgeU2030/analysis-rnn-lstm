{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQZ6OrZD4bDfwSp6k6sxMW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeU2030/analysis-rnn-lstm/blob/main/code-implementation/RNN_LSTM_TI2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment analysis - RNN and LSTM.\n",
        "\n",
        "Members\n",
        "\n",
        "- Luis Botero\n",
        "- Juan Medina\n",
        "- George Trujillo"
      ],
      "metadata": {
        "id": "eaUvW-UHtAMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Gather the dataset for sentiment analysis from the UCI Machine Learning Repository, specifically\n",
        "the Sentiment Labelled Sentences Data Set"
      ],
      "metadata": {
        "id": "FnidrNccQzat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We get the file from the download site\n",
        "!wget https://archive.ics.uci.edu/static/public/331/sentiment+labelled+sentences.zip\n",
        "\n",
        "# Unzip the obtained file\n",
        "!unzip sentiment+labelled+sentences.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5hZnPbwQywX",
        "outputId": "817ccf9f-fa40-43eb-978f-7e51e98dd79b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-20 17:20:10--  https://archive.ics.uci.edu/static/public/331/sentiment+labelled+sentences.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘sentiment+labelled+sentences.zip’\n",
            "\n",
            "sentiment+labelled+     [ <=>                ]  82.21K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-11-20 17:20:11 (564 KB/s) - ‘sentiment+labelled+sentences.zip’ saved [84188]\n",
            "\n",
            "Archive:  sentiment+labelled+sentences.zip\n",
            "   creating: sentiment labelled sentences/\n",
            "  inflating: sentiment labelled sentences/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/sentiment labelled sentences/\n",
            "  inflating: __MACOSX/sentiment labelled sentences/._.DS_Store  \n",
            "  inflating: sentiment labelled sentences/amazon_cells_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/imdb_labelled.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._imdb_labelled.txt  \n",
            "  inflating: sentiment labelled sentences/readme.txt  \n",
            "  inflating: __MACOSX/sentiment labelled sentences/._readme.txt  \n",
            "  inflating: sentiment labelled sentences/yelp_labelled.txt  \n",
            "  inflating: __MACOSX/._sentiment labelled sentences  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Preprocess the text data, including tokenization, lowercasing, and removing stopwords. Prepare the\n",
        "data for supervised learning (use NLTK)."
      ],
      "metadata": {
        "id": "XNbnxgJZdda3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# We load the data from the websites\n",
        "# amazon\n",
        "df_amazon = pd.read_csv('sentiment labelled sentences/amazon_cells_labelled.txt', sep='\\t', header=None)\n",
        "df_amazon.columns = ['sentence', 'label']\n",
        "\n",
        "# imdb\n",
        "df_imdb = pd.read_csv('sentiment labelled sentences/imdb_labelled.txt', sep='\\t', header=None)\n",
        "df_imdb.columns = ['sentence', 'label']\n",
        "\n",
        "# yelp\n",
        "df_yelp = pd.read_csv('sentiment labelled sentences/yelp_labelled.txt', sep='\\t', header=None)\n",
        "df_yelp.columns = ['sentence', 'label']\n",
        "\n",
        "# We mix the dataframes, reload the index, for no obtain duplicate indexes\n",
        "df = pd.concat([df_amazon, df_imdb, df_yelp], ignore_index=True)\n",
        "\n",
        "# We tokenize each sentence into individual words, convert words to lowercase, and remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df['sentence'] = df['sentence'].apply(lambda x: ' '.join([word.lower() for word in x.split() if word.isalnum() and word.lower() not in stop_words]))\n",
        "\n",
        "print(df['sentence'])\n"
      ],
      "metadata": {
        "id": "NV-rUa39dbw7",
        "outputId": "74a4e948-abca-4c64-e6ec-d48cbbd58e33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                   way plug us unless go\n",
            "1                                          good excellent\n",
            "2                                                   great\n",
            "3                   tied charger conversations lasting 45\n",
            "4                                                     mic\n",
            "                              ...                        \n",
            "2743                            think food flavor texture\n",
            "2744                                   appetite instantly\n",
            "2745                           overall impressed would go\n",
            "2746           whole experience think go ninja sushi next\n",
            "2747    wasted enough life poured salt wound drawing t...\n",
            "Name: sentence, Length: 2748, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We prepare the data for supervised learning\n",
        "# We divide the data in training and test\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['sentence'], df['label'], test_size=0.3, random_state=123)\n",
        "\n",
        "# Convert the text to numerical characteristics\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Convert the categorical to numerical characteristics\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n"
      ],
      "metadata": {
        "id": "9iEFgS87vgPq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}